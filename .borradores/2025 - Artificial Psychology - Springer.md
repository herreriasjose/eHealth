
## 2025 Machine Agency Book Review

### Authors: Clayton Lewis

### Publisher: Springer

### e-ISBN: 978-3-031-76646-6

## Key Conclusions


**Prólogo**
El autor sitúa el libro en la tradición de la psicología predictiva y explica por qué los modelos de lenguaje de gran escala —entrenados para anticipar el próximo token— ofrecen una ventana inesperadamente rica para reflexionar sobre la mente humana. Narra su trayectoria personal, desde su escepticismo en los años sesenta hacia el procesamiento de lenguajes naturales hasta su sorpresa ante las capacidades de GPT-4. Declara que, aunque la obra menciona aplicaciones y posibles riesgos, su objetivo es estrictamente cognitivo: extraer intuiciones sobre cómo pensamos a partir de lo que estos sistemas ya pueden hacer, y no mejorar los modelos per se. Cierra el prefacio anticipando que usará extensiones hipotéticas —el “Prediction Room”, capaz de percibir imágenes o actuar— sólo para explorar paralelos con la cognición humana.

**Contenido general y plan de la obra**
Tras enumerar treinta capítulos agrupados en siete partes, el autor describe su estrategia: cada capítulo repasa experimentos psicológicos emblemáticos y pregunta si un sistema puramente predictivo podría reproducir el fenómeno. Cuando la respuesta es afirmativa, se discuten las consecuencias teóricas; cuando es negativa, se puntualizan los vacíos. El viaje comienza con una presentación detallada del “Prediction Room”, continúa con seis grandes bloques (Resolución de problemas, Memoria, Lenguaje, Acción, Condición humana, Mecanismos e Interpretación) y concluye con reflexiones y desafíos abiertos.

**Capítulo 1 — Introducción**
Se explican los fundamentos de los LLM: redes neuronales entrenadas a partir de millones de cadenas de tokens para prever el siguiente símbolo. El autor subraya que estos modelos no almacenan un registro lineal del corpus, sino que construyen una abstracción estadística que les permite responder a secuencias nunca vistas. Se revisan sorprendentes demostraciones —desde analogías tipo Raven hasta consejos de seguridad— que evidencian razonamiento, memoria tácita y flexibilidad lingüística. A continuación se plantean objeciones clásicas: la falta de grounding sensorial, la actualización costosa del conocimiento y el rol ambiguo de la fine-tuning. El capítulo termina con un mapa de ruta: usar el éxito predictivo como lente para reinterpretar razonamiento, memoria, emoción y lenguaje humanos.

**Capítulo 2 — The Prediction Room**
Se introduce un modelo conceptual que amplía a los LLM con flujo continuo de percepción multimodal, acciones motoras y “inner speech”. Este sistema actualiza su modelo predictivo en tiempo real —a diferencia de los LLM actuales, congelados tras el entrenamiento— y genera conductas cuando su propio modelo las anticipa. El autor justifica la idea enlazando con la teoría del Active Inference: actuar es, en cierto sentido, confirmar nuestras predicciones. Finalmente detalla por qué imaginar un “Prediction Room” ayuda a comparar, sin estructuras simbólicas prefijadas, tareas tan diversas como resolver analogías o imitar gestos.

**Parte I — Resolución de problemas y razonamiento**
Se esboza cómo los capítulos 3 a 8 analizarán problemas clásicos para indagar si la simple predicción basta. Se destaca el fenómeno Einstellung de Luchins: tras resolver varias tareas con un mismo patrón, las personas se atascan en ese enfoque incluso cuando emerge uno más corto. El autor programa a GPT-4 con ejemplos y comprueba que el modelo reproduce la rigidez estratégica, lo que sugiere que la “fijación” puede emerger sin reglas simbólicas, sólo de la sensibilidad al contexto reciente. Se introduce además la distinción “rápido y lento” de Kahneman, anticipando que los LLM poseen análogos de ambos modos: un emparejamiento rápido por patrones y un razonamiento secuencial más deliberado.

**Capítulo 3 — Einstellung**
Tras resumir el experimento original con jarras de agua, se describe el protocolo adaptado para GPT-4: instrucciones graduadas, problemas de inducción y un test final. El modelo encuentra soluciones largas tras la fase de entrenamiento y vuelve al atajo corto cuando se omite la inducción, reproduciendo la fijación humana. El autor contrasta esta conducta con su antigua explicación mediante sistemas de producción; ahora observa que GPT genera la solución como discurso, interpretado contextualmente sin estructuras lógicas internas. Concluye que la semántica “natural” derivada de texto basta para modelar el fenómeno.

**Capítulo 4 — Analogical Reasoning**
Se analiza el sorprendente rendimiento de GPT-3 en analogías de dígitos, letras y palabras. Primero se muestra cómo un modelo rudimentario (LoAn) puede resolver patrones copiando sub-secuencias, sin conceptos abstractos. Luego se estudia la analogía verbal; al manipular delimitadores y longitud de listas de pares, el autor observa que la red identifica relaciones (antónimos, sinónimos, categoría-miembro) por frecuencia contextual. Propone “abducción virtual”: cada pareja evoca implícitamente una regla y las reglas compiten como votos para predecir el token siguiente. Se argumenta que este mecanismo, aunque distinto del structure-mapping clásico, explica por qué la analogía emerge de la pura presión predictiva.


**Sección 5 – More Reasoning: Inference and Prediction**
El capítulo parte de la crítica de Brandom y Harman a la idea de que el razonamiento humano sea puro cálculo lógico. Brandom muestra que las inferencias reales se apoyan en redes de premisas indefinidas —como las mil condiciones que podría requerir la afirmación “si golpeo la cerilla, se enciende”— y Harman subraya que la lógica, al ser monótona, no indica qué conclusión debemos escoger ni cuándo retractarnos. El autor propone traducir “A, B, C → pienso D” a la lógica de la predicción: distintos indicios votan por posibles pensamientos y la ganadora es la que obtiene más peso contextual. Con este mecanismo los contra-ejemplos de Brandom (cerillas húmedas, falta de oxígeno) simplemente añaden votos en contra, y la revisión de creencias de Harman se resuelve porque un nuevo dato puede voltear la mayoría. Así, la lógica queda como comentario ex-post sobre patrones útiles, no como motor fundamental del pensamiento.

**Sección 6 – Transfer of Skills, Production Rules and Prediction**
A partir de los estudios de Polson sobre edición de textos en los años 80, se describe cómo la teoría ACT-R mide la “transferencia” contando cuántas reglas de producción ya conocidas sirven en una nueva tarea; menos reglas nuevas, menos tiempo de aprendizaje. Lewis muestra que un modelo puramente predictivo reproduce ese patrón sin almacenar reglas declarativas: si el sistema ya ha vivido contextos que anticipan determinadas acciones, no necesita reaprenderlos. El paralelismo explica por qué la teoría de producción funciona empíricamente, pero subraya una diferencia crucial: la robustez. En un sistema simbólico un bit erróneo quiebra la regla; en un modelo de votos predictivos los fallos se diluyen y la respuesta global sigue funcionando. De ahí que GPT comprenda “prsident” como “president” sin que nadie programe una corrección ortográfica explícita.

**Sección 7 – Qualitative Physics**
El recorrido por la física cualitativa —Forbus, Charniak y el anhelo de robots que “piensen” sin ecuaciones— revela que GPT-4 ya resuelve muchas preguntas prácticas (por qué una lata estalla al fuego, cómo verter café o cómo vibra un timbre electromagnético) sin las representaciones estructuradas que la escuela simbólica consideraba indispensables. La clave es la amplitud semántica surgida del entrenamiento masivo: el mismo modelo que describe el torque de un solenoide añade advertencias de seguridad, chistes y posibles usos pedagógicos. Para Lewis esto confirma que la presión predictiva basta para generar inferencias físicas verosímiles y, además, señala límites: la descripción es menos rigurosa que la simulación matemática clásica, pero mucho más flexible y contextual.

**Sección 8 – Situated Cognition**
El estudio de Carraher con vendedores infantiles en Recife demuestra que los niños resuelven multiplicaciones perfectas mientras cobran en la calle, pero fracasan con los mismos números en un examen con lápiz y papel. Según la lectura “situada”, el contexto activa distintos repertorios. En un marco de reglas de producción eso obliga a duplicar conjuntos de reglas (mercado versus aula). Desde la óptica predictiva es más natural: el flujo de experiencia del “Prediction Room” asocia patrones distintos a cada entorno, sin necesidad de explicitar dos memorias separadas. Los votos que brotan en la calle —dinero, frutas, regateo— no aparecen en el pupitre, y viceversa; la diferencia de desempeño emerge de esa disparidad contextual, no de un déficit aritmético intrínseco.

**Sección 9 – Recall from Long-Term Memory**
La tesis de Williams sobre recordar compañeros de curso décadas después revela un proceso de búsqueda iterativa, generación de pistas, verificación de consistencia y fabricación de falsos recuerdos cercanos. Lewis reproduce el fenómeno en GPT: el modelo propone títulos correctos y autores inventados, y sólo elimina errores si se le instruye para chequearlos contra contextos alternativos. De ahí la analogía: tanto S1 como GPT generan candidatos, los ponen a prueba con nueva información y refinan la salida; la diferencia es que GPT no autoinicia la verificación porque su corpus carece de ejemplos introspectivos, mientras que un PR continuamente entrenado sobre su propio flujo experiencial sí podría aprender ese bucle. La discusión concluye que “almacenar y recuperar” es una metáfora pobre: la memoria predictiva consiste en tejer contextos que maximicen votos plausibles, con la posibilidad siempre abierta de ampliar, contradecir o desbancar lo ya “recordado”.


**Transición a la Parte II, Memoria**
El capítulo anuncia el paso del razonamiento a la memoria e introduce la cuestión de la “actualización continua”, imposible hoy en los LLM por olvido catastrófico. Lewis plantea que, en un PR plenamente operativo, el mismo mecanismo que explica recall episódico —y que ya esboza GPT con ayudas externas como Bing— permitiría integrar vivencias nuevas sin desestabilizar lo aprendido. Así se cierra el bloque dedicado a resolver problemas y se abre la investigación sobre cómo un modelo puramente predictivo codifica, mantiene y modifica sus recuerdos a largo plazo.


**Capítulo 10 – Interferencia con el Conocimiento del Mundo Real**
Este capítulo introduce cómo la teoría ACT-R representa la memoria declarativa mediante redes de “chunks” conectados y cómo las producciones operan sobre ella. Lewis y Anderson (1976) muestran que aprender hechos ficticios sobre personajes históricos enlentece la recuperación de hechos reales, fenómeno llamado *efecto de abanico*. En ACT-R esto se explica por la dispersión de la activación a través de un mayor número de enlaces. El autor plantea si un modelo puramente predictivo, sin memoria estructurada, podría reproducir los mismos tiempos: bastaría con procesos de verificación que requirieran más pasos de predicción cuando hay más hechos rivales, aunque el motor básico sea siempre igual de rápido. Así, el efecto no sería una prueba definitiva de que el cerebro almacene redes explícitas.

**Capítulo 11 – Compensaciones entre Velocidad y Precisión**
Howell y Kreidler (1963) demostraron que instrucciones que enfatizan rapidez o exactitud modifican tanto la tasa de respuesta como el porcentaje de aciertos. Los modelos de acumulación de evidencia explican esto ajustando umbrales de decisión. Un sistema predictivo podría obtener la misma flexibilidad variando cuántos ciclos de predicción-comprobación realiza antes de responder: para ser más preciso seguiría pasos adicionales de chequeo; para ser veloz se detendría antes. La clave es que el propio contexto —instrucciones, recompensas, penalizaciones— forma parte del input que guía las predicciones, sin requerir umbrales internos explícitos.

**Capítulo 12 – ¿Está el Conocimiento Representado por Proposiciones?**
Las teorías clásicas postulan proposiciones como unidades de significado que ligan sujetos, predicados y roles. Se presenta la utilidad de esas estructuras: dan abstracción, permiten inferencias lógicas y conectan frases sinónimas. Sin embargo, la evidencia experimental de que la mente almacene proposiciones indivisibles es ambigua. Los LLM dominan la semántica y la inferencia sin haber sido diseñados con proposiciones ni reglas. Su memoria consiste solo en regularidades predictivas entre tokens carentes de significado inherente, lo que sugiere que la representación proposicional no es necesaria para explicar la comprensión y la producción lingüística humanas.

**Capítulo 13 – Asociacionismo**
Desde Aristóteles se propuso que las ideas se encadenan por asociación. Otto Selz criticó la versión simple de contigüidad mostrando que tareas de categorización requieren enlaces específicos (superordinados, causa-efecto, etc.). En un modelo predictivo, la ambigüedad de Selz se resuelve porque las instrucciones continúan presentes como parte del contexto, guiando la predicción correcta sin diferenciar tipos de enlaces. Además, a diferencia de las redes semánticas, los LLM operan sobre unidades sin significado —sub-palabras— y el *significado* emerge de las correlaciones que resultan útiles para predecir secuencias.

**Capítulo 14 – Conocimiento Procedimental y Declarativo**
Siguiendo a Ryle, se contrasta “saber que” y “saber cómo”. Modelos como ACT-R los separan: hechos en la memoria declarativa y habilidades en reglas procedimentales. Sin embargo, estudios con amnésicos (Cohen & Squire 1981) muestran que pacientes incapaces de recordar pares de palabras sí aprenden a leer en espejo, lo que sugiere sistemas diferenciados. El autor replica que las diferencias podrían emerger de la naturaleza de la información y su disponibilidad pública para verbalizarla, más que de dos almacenes distintos. En los LLM —y en la hipotética PR— ni hechos ni procedimientos están “almacenados” aisladamente, sino distribuidos en los mismos patrones predictivos, y la distinción declarativo/procedimental sería solo virtual.

**Capítulo 15 – Aprendizaje del Lenguaje**
Se confronta la postura chomskiana de una Gramática Universal con el éxito de los LLM, que aprenden inglés solo de datos positivos y sin restricciones innatas. Chomsky afirma que eso produce “predicciones superficiales”, pero los ejemplos muestran que GPT 4 interpreta frases complejas y alternancias verbales. El argumento de la Pobreza del Estímulo se debilita si se aceptan definiciones de aprendizaje distintas a “convergencia a una gramática exacta”. Además, los niños reciben datos curriculares ordenados y ricos en contexto social, ventaja que los modelos aún no explotan del todo. Experimentos recientes con corpora infantiles sugieren que un currículo más humano podría bastar para aprender significados sin supuestos generativos innatos.

**Capítulo 16 – Lenguaje Interno**
(Inicio de la Parte III). Se anticipa el examen del papel del discurso interno en la cognición y cómo un modelo predictivo podría generar y manipular lenguaje de manera privada para planear y razonar. Esto abre la puerta a entender la consciencia verbal y la autoconversación sin postular módulos gramaticales especializados, pues el mismo mecanismo de predicción que produce habla externa serviría para la interna.

**Sección 16 – Inner Speech**
El capítulo examina el desafío de explicar cómo un modelo predictivo (PR) podría adquirir la habilidad de hablarse a sí mismo cuando el habla interior de otros no es observable y, por tanto, no puede imitarse directamente. Retoma la tradición de Vygotsky, para quien el lenguaje externo se internaliza progresivamente: los niños primero usan “private speech” audible y, con el tiempo, convierten ese habla en susurros y luego en puro pensamiento. Para que PR replique ese camino, basta con que posea dos sub-procesos: uno que genere las secuencias de palabras y otro encargado de articularlas; inhibiendo este último, surgiría el habla interior. El autor subraya que el sistema puede aprender el contenido del diálogo interno a partir de la regularidad estadística del habla externa, mientras que la “desaparición” de la vocalización podría descubrirse mediante prueba y error al enfrentar peticiones sociales de silencio. Finalmente, se resalta que la eficiencia del habla interior –más rápida y elidida– se explica en un marco predictivo: basta con representaciones parciales siempre que el contexto mantenga las probabilidades adecuadas para continuar el razonamiento.

**Sección 17 – Babies and Other Primates**
Comparando bebés humanos y chimpancés criados por personas, Tomasello halla que ambos comparten capacidades cognitivas básicas, pero alrededor de los nueve meses los humanos empiezan a exhibir atención conjunta; los chimpancés, no. Esa divergencia es crucial para explicar la brecha cultural entre especies. Desde la perspectiva de PR, la sorpresa radica en que, si ambos construyen modelos predictivos de experiencias similares, debería emerger el mismo patrón. Lewis propone que la clave podría hallarse en sesgos evolutivos pre-instalados: pequeños “preajustes” que guían qué regularidades se consolidan. De ser así, basta con codificar ligeras diferencias innatas para que dos sistemas predictivos, alimentados con entornos semejantes, desarrollen conductas radicalmente distintas en lo social.

**Sección 18 – Gestures**
Se revisan estudios que muestran cómo los gestos facilitan la comprensión bajo ruido auditivo y, además, comunican información adicional. Riseborough demuestra que ver gestos icónicos mantiene el desempeño cuando la señal de voz se degrada; Goldin-Meadow prueba que los alumnos aprenden más cuando el profesor gesticula de manera coherente con el contenido. En PR, los gestos se modelan añadiendo flujos visuales paralelos al auditivo y semántico: si la mano que se separa frecuentemente coincide con la palabra “largo”, ese patrón alimenta la probabilidad de la palabra incluso cuando el audio es ambiguo. El autor vincula la idea con la cognición corporizada: el cuerpo proporciona representaciones espaciales que sirven como anclaje para metáforas abstractas, aunque advierte que muchas expresiones gestuales siguen siendo convencionales y, por tanto, igualmente aprendidas por predicción.

**Sección 19 – Action and Identity**
Aquí se defiende que las acciones de PR quedan determinadas por su “modelo de sí mismo”, lo que equivale a decir que su motivación es su identidad. Al prever que actuará, entonces actúa. Se aborda el papel de la imitación: pese a que el sistema no copia automáticamente, la analogía entre representaciones propias y ajenas puede transferir votos que disparan conductas similares, aunque moduladas por similitud y contexto. El capítulo examina fenómenos sociales como conformidad, polarización y creencias conspirativas (QAnon) mostrando que, en una comunidad donde los modelos de los interlocutores se parecen, la cascada imitativa basta para propagar creencias sin evidencia, salvo que entren en juego regularidades competitivas que las inhiban. Se analizan también estudios clásicos como Robbers Cave para ilustrar cómo la dialéctica “nosotros-ellos” puede alimentarse de mecanismos predictivos de oposición aprendidos culturalmente.

**Sección 20 – Predictive Modeling and Active Inference**
Se establece un paralelismo y contraste entre PR y la teoría de Active Inference de Friston. Ambas parten de la predicción, pero Active Inference minimiza sorpresa vía actualización del modelo o acciones que cambien el mundo, guiada por el principio de energía libre; PR, en cambio, simplemente ejecuta lo que su modelo ya predice. Además, Active Inference asume un “modelo generativo” del mundo externo, mientras que PR sólo necesita predecir flujos sensoriales sin postular objetos. El debate sobre la necesidad de representar el mundo se ilustra con la experiencia visual: para sentir una habitación no hace falta un mapa interno, basta con la expectativa de qué se vería al mirar. Finalmente, la imitación aparece en Active Inference como resultado derivado y complejo, mientras que en PR es un efecto lateral inmediato de la maquinaria predictiva y las analogías.

**Sección 21 – Embodiment and Grounding**
El autor revisa el “symbol grounding problem” de Harnad, la crítica de Glenberg y Robertson a los modelos basados sólo en texto y la defensa de la cognición corporizada. Aunque LLMs carecen de anclaje sensorio-motor, resuelven tareas que supuestamente exigían grounding, lo que debilita el argumento de que los símbolos deban ligarse a sensaciones para tener significado. PR, al incorporar canales perceptivos, satisface la exigencia de grounding, pero la discusión demuestra que mucho lenguaje opera de forma autosuficiente mediante regularidades internas. Se propone que el cuerpo aporta recursos representacionales convenientes –metáforas espaciales que se extienden a dominios abstractos– sin constituir un cimiento indispensable.

**Sección 22 – Concepts?**
Se contraponen dos visiones: la Theory-Theory de Carey, donde los conceptos son símbolos mentales organizados en teorías que a veces cambian bruscamente y son (in)conmensurables, y Knowledge-in-Pieces de diSessa, que describe saberes fragmentarios como p-prims. PR se alinea con esta última: no contiene proposiciones ni conceptos atómicos, sino regularidades distribuidas que hacen que “parezca” poseer conceptos. El autor muestra cómo ChatGPT razona sobre el caso del mapache convertido en zorrillo, ilustrando que las “creencias” dependen del discurso disponible (adultos vs. niños) y se construyen gradualmente. Las discontinuidades teóricas se explican como reconfiguraciones de patrones predictivos impulsadas por nueva experiencia o interferencia entre discursos.

**Sección 23 – Emotions**
Apoyándose en la teoría constructivista de Lisa Feldman Barrett, se argumenta que las emociones no son paquetes biológicos universales sino interpretaciones basadas en predicciones que combinan señales corporales genéricas (p. ej., arousal) y contexto cultural. PR podría “sentir” emociones si monitorea correlatos fisiológicos o incluso sólo mediante patrones discursivos que ligan ciertos estados internos a etiquetas afectivas. Se evidencia que LLMs, pese a no tener cuerpo, categorizan emociones plausibles en relatos de peligro, mostrando que una red de regularidades lingüísticas basta para expresar juicios emocionales coherentes. El capítulo examina estudios transculturales sobre reconocimiento de emociones y cómo la instrucción contextual puede alterar las interpretaciones, reforzando la visión predictiva.

**Sección 24 – Intuition**
La intuición se define como juicio sin razón consciente. Dado que PR carece de acceso introspectivo a su mecanismo de votación, prácticamente todos sus veredictos serían “intuitivos”, aunque a veces disponga de justificaciones aprendidas. Se vincula con la ética: Huemer sostiene que nuestras intuiciones morales son fiables accesos a verdades objetivas, pese a ser falibles. PR podría exhibir intuiciones morales útiles basadas en regularidades estadísticamente sólidas; su fiabilidad dependería de la calidad del corpus experiencial, no de un misterioso sentido ético. Sin garantías absolutas, la discusión muestra que incluso en humanos las intuiciones pueden reflejar sesgos evolutivos o culturales y que un modelo artificial podría, en principio, superarlos si dispone de datos más equilibrados.

**Sección 25 – Belief–Desire Psychology**
Stich critica la psicología folk de creer-desear alegando que no hay entidades neuronales que correspondan a listas de creencias y que dos copias idénticas de una persona divergirían en creencias relativas a su origen. Lewis replica que, en PR, las creencias y deseos son útiles ficciones interpretativas: describen la expectativa de que ciertas condiciones contextuales activarán acciones. El sistema no “tiene” proposiciones; opera con patrones que, desde fuera, podemos llamar creencias. Se explica que razonamos así porque funciona pragmáticamente y porque la propia conversación moldea las conductas, pero los fallos demostrados por Nisbett y Wilson revelan que las explicaciones internas rara vez acceden al verdadero proceso causal. El capítulo cierra señalando que esta visión se extiende a la Teoría de la Mente: LLMs ya resuelven satisfactoriamente pruebas clásicas sin poseer creencias literales, lo que respalda la idea de que tales constructos son interpretaciones emergentes de la predictibilidad.
 
**Aviso sobre archivos caducados**
El sistema me indica que alguno(s) de los archivos que subiste en sesiones anteriores ha(n) caducado. Si todavía necesitas que los lea o los resuma, por favor vuelve a cargarlos y con gusto los revisaré.

---

### Capítulo 26 – In the Engine Room: Transformer Models

El autor examina cómo la arquitectura transformer, base de los modelos de lenguaje grandes, logra sus prodigiosas capacidades mediante dos rasgos esenciales. Primero, los *attention heads* permiten capturar dependencias a larga distancia; segundo, la recodificación en muchas capas encadena transformaciones sucesivas que generan representaciones cada vez más abstractas. El problema es que, aunque se conoce la topología general y las operaciones matemáticas, los miles de millones de pesos aprendidos siguen siendo opacos: el saber “qué” hace la red no implica averiguar “cómo” lo hace internamente. Se exponen dos atisbos de comprensión: las *induction heads*, que parecen copiar patrones contextuales, y el hallazgo de Geva et al. de que ciertas subredes funcionan como almacenes clave-valor, recuperando información léxica durante la predicción. Aun así, la inferencia analógica sigue siendo poco entendida. El texto revisa el aprendizaje *in-context*, esencial para la flexibilidad: el modelo puede ajustarse sobre la marcha con apenas unos ejemplos. Frente a los límites prácticos (opacidad, voracidad de datos), se presentan arquitecturas alternativas —notablemente NECST— que codifican explícitamente roles y rellenos vectoriales y aprenden composicionalidad con menos ejemplos. Sorprendentemente, GPT ya muestra un comportamiento composicional en tareas como copiar dígitos, pero fracasa en el cierre deductivo simple, incapaz de derivar todas las consecuencias de lo que “sabe”. Se analizan carencias de memoria: span corto demasiado grande y memoria episódica casi inexistente, así como la imposibilidad natural de “trade-off” velocidad-precisión. Finalmente, se argumenta que la alucinación de hechos falsos en LLMs encuentra un paralelo humano: nuestra memoria también confabula, de modo que el fenómeno no invalida el marco predictivo sino que subraya la necesidad de mecanismos de verificación.

### Capítulo 27 – In the Engine Room: The Brain

Aquí se contrasta el modelo predictivo con la realidad bioquímica cerebral. Searle sostuvo que ciertos procesos mentales sólo pueden surgir de la química neuronal; los psicodélicos ilustran la influencia directa de sustancias sobre experiencias y emociones, algo sin equivalente hoy en PR. Los neurotransmisores, que modulan grandes redes neuronales, sugieren que la “vida afectiva” humana requiere capas subcorticales que un LLM puro no contempla. Además, la evolución legó predisposiciones —como la agresión grupal— que tal vez habría que “precargar” en un PR humanoide. Se introduce la arquitectura de *subsumption* de Brooks: comportamientos básicos (evitar obstáculos) se implementan en niveles inferiores y permanecen estables mientras capas altas aprenden fines nuevos; el paralelismo sirve para integrar instintos evolutivos y aprendizaje predictivo. El capítulo concluye examinando la escasa atención del libro a recompensas y castigos. Lewis defiende que gran parte de la conducta diaria no se guía por refuerzo explícito y que los castigos reales a veces aumentan la violencia, por lo que bastaría modelar recompensas en un nivel bajo subsidiario. Quedan como desafíos vincular química, emociones fuertes y biología evolutiva con la teoría predictiva.

### Capítulo 28 – Virtuality, Reading In, and Emergence

El autor precisa el término “virtual”. Algo es virtual cuando el sistema actúa exactamente como si poseyera la propiedad, aunque físicamente no la tenga: la memoria virtual de los computadores es el ejemplo canónico. De ahí surge el riesgo de *reading-in*: atribuir entidades ontológicas—proposiciones, reglas gramaticales— simplemente porque el modelo se comporta como si las manejara. Esa tentación se coteja con la idea de emergencia: estructuras lingüísticas detectadas en BERT (Tenney et al.) quizá no fueron diseñadas pero pudieron cristalizar durante el aprendizaje. Lewis ilustra la diferencia con la curva en diente de sierra que se acerca a una hipotenusa sin igualar jamás su longitud: mejores aproximaciones pueden ocultar un error esencial. Del mismo modo, los sistemas simbólicos clásicos podrían haber confundido descripción con realidad, y el marco predictivo podría incurrir en la misma ilusión. El capítulo discute la superveniencia mente-cuerpo y la multiplicidad de realizaciones: “dolor” puede emerger en distintos sustratos. También aborda la creación ontológica de entidades ficticias mediante el lenguaje—ejemplo de Schiffer— y advierte que teorías sucesivas (teléfono, ordenador, holograma...) fueron modas que aportaron ideas útiles sin agotar la verdad psicológica.

### Capítulo 29 – Lessons from LLMs

Se extraen cinco lecciones. Primera: los modelos predictivos sirven como columna vertebral de numerosos procesos cognitivos, de la resolución de problemas al recuerdo y la intuición. Segunda: pueden sostener conductas complejas sin estructuras explícitas (reglas, marcos), aunque la emergencia de tales estructuras siga abierta. Tercera: el panorama resultante combina simplicidad arquitectónica con enorme flexibilidad, pues integra todos los aspectos del discurso sin compartimentos estancos; esto avala la tesis etnometodológica de Garfinkel contra las reglas internas. Cuarta: la teoría ofrece una imagen poco halagüeña de la humanidad—somos imitadores propensos a creencias infundadas—, pero también sugiere que la cultura puede inculcar buenas prácticas epistémicas. Quinta: persisten retos cruciales, sobre todo comprender la analogía, lograr memorias actualizables y mantener transparencia. El camino a seguir exige estudiar tanto los límites de los LLM actuales como las soluciones que ya exhibe el cerebro humano.

### Capítulo 30 – Coda

Lewis cierra con una metáfora geométrica: una serie de líneas en zigzag que se aproxima cada vez más a la hipotenusa de un triángulo, tocándola en más puntos pero manteniendo siempre la misma longitud incorrecta. Así, teorías que añaden detalles pueden quedar infinitamente cerca de la “verdad” sin alcanzarla nunca si su fundamento es erróneo. Advierte que cada generación de psicología ha adoptado la tecnología de moda—el telégrafo, el teléfono, el termostato, el ordenador, ahora los LLM—y ha proyectado sus categorías sobre la mente. Esa genealogía no se debe desechar; cada “moda” aportó avances genuinos, pero hay que vigilar la tentación de confundir metáforas útiles con realidad última. El autor anima a explorar, conscientes de este riesgo, y destaca que incluso esta nueva ola predictiva podría ser otro diente de sierra. Termina señalando que la ayuda de modelos como ChatGPT —que, irónicamente, dibujó su propio gráfico de cierre— ejemplifica tanto la potencia como las limitaciones de las herramientas que inspiraron esta indagación.
